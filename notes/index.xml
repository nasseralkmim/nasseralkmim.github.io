<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Nasser&#39;s personal website</title>
    <link>https://nasseralkmim.github.io/notes/</link>
    <description>Recent content on Nasser&#39;s personal website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>© {year}</copyright>
    <lastBuildDate>Thu, 17 Dec 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://nasseralkmim.github.io/notes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>2020 Books review</title>
      <link>https://nasseralkmim.github.io/notes/books-2020/</link>
      <pubDate>Thu, 17 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://nasseralkmim.github.io/notes/books-2020/</guid>
      <description>Books list    Kafka on the shore  Murakami escreve bem, não sou especialista em romances mas a forma como ele leva a narrativa realmente é cativante e te prende. Esse é o segundo livro que leio desse autor, e fica claro uma tendência na relação do autor com arte em geral. É muito interessante ver como a erudição do autor é projetada nós personagens quando eles discutem alguma peça musical ou literária.</description>
    </item>
    
    <item>
      <title>Topic modeling</title>
      <link>https://nasseralkmim.github.io/notes/topic_modelig/</link>
      <pubDate>Fri, 11 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://nasseralkmim.github.io/notes/topic_modelig/</guid>
      <description>Introduction  Topic modeling is a form of semantic analysis, a step forwarding finding meaning from word counts. This analysis allows discovery of document topic without trainig data. It involves counting words and grouping similar word patterns to describe the data.
  Latent Dirichlet Allocation   Latent Dirichlet Allocation (LDA) is one topic modeling technique. It can infer the probability distribution of words for each topic, characterizing it.</description>
    </item>
    
    <item>
      <title>NLP with Python</title>
      <link>https://nasseralkmim.github.io/notes/nlp-with-python/</link>
      <pubDate>Wed, 09 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://nasseralkmim.github.io/notes/nlp-with-python/</guid>
      <description>Introduction  This is an attempt to compile different sources of informations about Natural Language processing with python.
  Techiniques used in NLP  Tokenization  Converts text into segments (n-grams) that could represent a word, two words or more. During this step, usually it is performed some kind of vocabulary reduction such as normalization, stemming, lemmatization and removing stop words.
Tools    NLTK: string processing library, built by academics.</description>
    </item>
    
    <item>
      <title>Text processing with spaCy</title>
      <link>https://nasseralkmim.github.io/notes/text-processing-with-spacy/</link>
      <pubDate>Wed, 09 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://nasseralkmim.github.io/notes/text-processing-with-spacy/</guid>
      <description>Introduction   spaCy is a library for Natural Language Processing (NLP) in python. It offers multiple solutions for text processing such as tokenization, named entity recognition, word vectors, part of speech tagging. The alternative is the library NLTK which seems to be used mostly in academia whereas spaCy is recommended for production use.
  Load the language model  We need to download the models for the language with python -m spacy download model-name.</description>
    </item>
    
    <item>
      <title>Question classifier preprocessing</title>
      <link>https://nasseralkmim.github.io/notes/question-classifier/</link>
      <pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://nasseralkmim.github.io/notes/question-classifier/</guid>
      <description>Introduction   An example from the book by Aman Kedia. About label encoding.
  Loading data  import pandas as pd data = open(&amp;#34;train_1000-label.txt&amp;#34;, &amp;#39;r&amp;#39;) train_raw = pd.DataFrame(data.readlines(), columns=[&amp;#39;data&amp;#39;]) print(train_raw.head())  data 0 DESC:manner How did serfdom develop in and the... 1 ENTY:cremat What films featured the character ... 2 DESC:manner How can I find a list of celebriti... 3 ENTY:animal What fowl grabs the spotlight afte... 4 ABBR:exp What is the full form of .</description>
    </item>
    
    <item>
      <title>Sentiment analysis on IMDB dataset</title>
      <link>https://nasseralkmim.github.io/notes/sentiment-analysis/</link>
      <pubDate>Fri, 04 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://nasseralkmim.github.io/notes/sentiment-analysis/</guid>
      <description>Introduction   This is as exemple from the excellent book by François Chollet on deep learning. My idea here is to further detail the explanation with the code output, which the book does not contain. And since tensorflow 2.0 was released, I will be using tf.keras instead.
 The goal of this example is
 The steps are
  Load data   Load imdb data from keras datasets.</description>
    </item>
    
    <item>
      <title>First post in HUGO</title>
      <link>https://nasseralkmim.github.io/notes/first-hugo-post/</link>
      <pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://nasseralkmim.github.io/notes/first-hugo-post/</guid>
      <description>Introduction  Meta post about how I do it. I&amp;#39;m using org-mode inside emacs and Hugo.
  Images from URL  Use the html export block from org mode. In order to override the theme css for images display I manually inserted the html with a different style=&amp;#34;display: revert; max-width: fit-content;&amp;#34;. (this depends on the theme you choose)
  Graphs from interactive code blocks saved on /static/images   Using emacs-jupyter together with org-mode babel.</description>
    </item>
    
    <item>
      <title>2019 Books review</title>
      <link>https://nasseralkmim.github.io/notes/books2019/</link>
      <pubDate>Sat, 05 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://nasseralkmim.github.io/notes/books2019/</guid>
      <description>Books list    Never let me go  Ficção sobre uma realidade distópica onde nós criamos clones para suprir nossas necessidades de órgãos. Mas quem são esses clones? A narrativa é contada por um deles. Cheio de mistérios e revelações, o livro te prende até o fim. Nas entrelinhas é um livro sobre questões existênciais e como aceitar o invevitavel. &amp;#34;Ishiguro wanted us to &amp;#39;know without knowing&amp;#39;&amp;#34;</description>
    </item>
    
    <item>
      <title>2018 Books review</title>
      <link>https://nasseralkmim.github.io/notes/books2018/</link>
      <pubDate>Thu, 31 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://nasseralkmim.github.io/notes/books2018/</guid>
      <description>Books list    The Dao of capital  Esse livro mostra a estratégia de investimento do Mark Spitznagel. Ele utiliza de operações assimétricas para ter um grande upside em face à eventos inesperados (ou eventos esperados e previsíveis mas que não se sabe quando ocorrerão). No decorrer do livro ele explica a estratégia adotada com exemplos das coníferas
  The 4-hour workweek  Excelente livro para remoldar a forma de pensar o tradicional caminho corporativista.</description>
    </item>
    
    <item>
      <title>2017 Books review</title>
      <link>https://nasseralkmim.github.io/notes/books2017/</link>
      <pubDate>Mon, 02 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://nasseralkmim.github.io/notes/books2017/</guid>
      <description>Books list    Antifragile  Livro essencial para todos. A maneira como o Taleb escreve é muito interessante e me agrada muito. O livro te explica como certas circunstâncias podem ser favorecidas quando um agente stressor ocorre. Essa ideia é aplicada em finanças e também nos mais diversos expectros da vida. Meu capitulo favorito foi sobre &amp;#34;ensinando pássaros a voar&amp;#34; que trata a ciência, pelo menos a ciência do séc XIX, como uma atividade assimétrica, ou seja, através do &amp;#34;thinkering&amp;#34;, ou experimentos que custam pouco (pequeno downside) mas que tem grande potencial (upside), a ciência era feita de forma eficiênte de baixo para cima (bottom up).</description>
    </item>
    
  </channel>
</rss>
