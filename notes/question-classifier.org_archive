#    -*- mode: org -*-


Archived entries from file c:/Users/nasse/OneDrive/nasser-website/content/notes/question-classifier.org


* One hot encoding
:PROPERTIES:
:ARCHIVE_TIME: 2020-12-08 Tue 16:26
:ARCHIVE_FILE: ~/OneDrive/nasser-website/content/notes/question-classifier.org
:ARCHIVE_OLPATH: Preprocessing
:ARCHIVE_CATEGORY: question-classifier
:END:

As an alternative to label encoding we can use one-hot encoding.
Why? because the naive label encoding might add a unwanted pattern to the data, the progressions.
*One-hot encoding* creates a column for each category.

We can do that with Scikit-Learn or TensorFlow (with Keras).

In order to use Keras preprocessing API the text input should be a single input.
We do that with pandas string functions.
The we use the one hot method for preprocessing text in keras, which *is not a one-hot encoding* but simply a label encoding.
The we perform the 

#+begin_src jupyter-python
from tensorflow.keras import utils, preprocessing
print(classes.str.cat(sep=' '), classes.size)
enc_classes = preprocessing.text.one_hot(classes.str.cat(sep=' ').lower(),
                                         n=classes.size,
split=' ')
print(enc_classes)
one_hot_classes = utils.to_categorical(enc_classes,
                                      classes.size) # number of classes
print(one_hot_classes)
#+end_src

#+RESULTS:
: DESC ENTY ABBR HUM NUM LOC 6
: [1, 2, 5, 4, 3, 1]
: [[0. 1. 0. 0. 0. 0.]
:  [0. 0. 1. 0. 0. 0.]
:  [0. 0. 0. 0. 0. 1.]
:  [0. 0. 0. 0. 1. 0.]
:  [0. 0. 0. 1. 0. 0.]
:  [0. 1. 0. 0. 0. 0.]]


It seems its giving the same integer to multiple categories.
Let's tray the =Tokenizer()= utility class.

#+begin_src jupyter-python
tokenizer = preprocessing.text.Tokenizer(num_words=classes.size)
print(classes.to_list())
print(tokenizer.texts_to_matrix(classes.to_list()))
#+end_src

#+RESULTS:
: ['DESC', 'ENTY', 'ABBR', 'HUM', 'NUM', 'LOC']
: [[0. 0. 0. 0. 0. 0.]
:  [0. 0. 0. 0. 0. 0.]
:  [0. 0. 0. 0. 0. 0.]
:  [0. 0. 0. 0. 0. 0.]
:  [0. 0. 0. 0. 0. 0.]
:  [0. 0. 0. 0. 0. 0.]]
