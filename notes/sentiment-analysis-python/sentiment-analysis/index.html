<!DOCTYPE html>
<html lang="en-us">
<head>
  <link rel="preload" href="/lib/font-awesome/webfonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="preload" href="/lib/font-awesome/webfonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="preload" href="/lib/font-awesome/webfonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="preload" href="/lib/JetBrainsMono/web/woff2/JetBrainsMono-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title> Sentiment analysis on IMDB dataset | Notes</title>
  <link rel = 'canonical' href = 'https://nasseralkmim.github.io/notes/sentiment-analysis-python/sentiment-analysis/'>
  
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="robots" content="all,follow">
  <meta name="googlebot" content="index,follow,snippet,archive">
  <meta property="og:title" content="Sentiment analysis on IMDB dataset" />
<meta property="og:description" content="Introduction   This is as exemple from the excellent book by François Chollet on deep learning. My idea here is to further detail the explanation with the code output, which the book does not contain." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://nasseralkmim.github.io/notes/sentiment-analysis-python/sentiment-analysis/" /><meta property="article:section" content="notes" />
<meta property="article:published_time" content="2020-12-04T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2020-12-04T00:00:00&#43;00:00" />


  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Sentiment analysis on IMDB dataset"/>
<meta name="twitter:description" content="Introduction   This is as exemple from the excellent book by François Chollet on deep learning. My idea here is to further detail the explanation with the code output, which the book does not contain."/>

  
  
    
  
  
  <link rel="stylesheet" href="https://nasseralkmim.github.io/css/styles.4c2b9aa1d874d6766f554b2d404e8fd62ab4761f51ee9b3f358d12e81e7fa43a1b4378db995bc1926bbe5ed98c060be5e7bd4f2470504cf94f22b4b3a74e62b6.css" integrity="sha512-TCuaodh01nZvVUstQE6P1iq0dh9R7ps/NY0S6B5/pDobQ3jbmVvBkmu&#43;XtmMBgvl571PJHBQTPlPIrSzp05itg=="> 

  
  
  
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  

  
<link rel="icon" type="image/png" href="https://nasseralkmim.github.io/images/favicon.ico" />

  
  
  
    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-74704246-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
  
  
</head>

<body class="max-width mx-auto px3 ltr">
  <div class="content index py4">

    <header id="header">
  <a href="https://nasseralkmim.github.io/">
  
    <div id="logo" style="background-image: url(https://nasseralkmim.github.io/images/logo.png)"></div>
  
  <div id="title">
    <h1>Notes</h1>
  </div>
  </a>
  <div id="nav">
    <ul>
      <li class="icon">
        <a href="#" aria-label="Menu"><i class="fas fa-bars fa-2x" aria-hidden="true"></i></a>
      </li>
      
        <li><a href="/">about</a></li>
      
        <li><a href="/notes">notes</a></li>
      
    </ul>
  </div>
</header>



    
<article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <div class="content" itemprop="articleBody">
  
    
<div id="outline-container-headline-1" class="outline-2">
<h2 id="headline-1">
Introduction
</h2>
<div id="outline-text-headline-1" class="outline-text-2">
<p>
This is as exemple from the excellent book by François Chollet on deep learning.
My idea here is to further detail the explanation with the code output, which the book does not contain.
And since tensorflow 2.0 was released, I will be using <code class="verbatim">tf.keras</code> instead.</p>
<p>
The goal of this example is</p>
<p>
The steps are</p>
</div>
</div>
<div id="outline-container-headline-2" class="outline-2">
<h2 id="headline-2">
Load data
</h2>
<div id="outline-text-headline-2" class="outline-text-2">
<p>
Load imdb data from keras datasets.
This dataset represents 25000 movie reviews and they are labeled by sentiment.
The format is a list of of length 25000 and each entry is another list that represent a review.
Integers represent the frequency of the word, so we are considering the top 1000 most frequent ones.
We can see the sentiment values is 0 or 1.</p>
<div class="src src-python">
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#080;font-weight:bold">from</span> <span style="color:#b06;font-weight:bold">tensorflow.keras</span> <span style="color:#080;font-weight:bold">import</span> datasets, preprocessing

max_features = <span style="color:#00d;font-weight:bold">10000</span>           <span style="color:#888"># number of words</span>

(train_data, train_targ), (test_data, test_targ) = datasets.imdb.load_data(
    num_words=max_features) <span style="color:#888"># limits to the most common ones </span>

<span style="color:#080;font-weight:bold">print</span>(<span style="color:#038">len</span>(train_data), train_data[<span style="color:#00d;font-weight:bold">0</span>])
<span style="color:#080;font-weight:bold">print</span>(<span style="color:#038">set</span>(train_targ))
<span style="color:#080;font-weight:bold">print</span>(train_targ)</code></pre></div>
</div>
<pre class="example">
25000 [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]
{0, 1}
[1 0 0 ... 0 1 0]
</pre>
<p>
With this bunch of numbers is hard to grasp the real meaning of this data.
Fortunately we can convert it back to text using the index of words also provided by the dataset.</p>
<div class="src src-python">
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">index = datasets.imdb.get_word_index()
<span style="color:#080;font-weight:bold">print</span>(<span style="color:#038">list</span>(index.items())[:<span style="color:#00d;font-weight:bold">5</span>])</code></pre></div>
</div>
<pre class="example">
[(&#39;fawn&#39;, 34701), (&#39;tsukino&#39;, 52006), (&#39;nunnery&#39;, 52007), (&#39;sonja&#39;, 16816), (&#39;vani&#39;, 63951)]
</pre>
<p>
We can see that the word index is a dictionary where the key is the word and the value is the integer representation.
Because we have the values and want the keys, it is useful to invert this dictionary so we can entry and integer and get out a word.</p>
<div class="src src-python">
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">index_word = {value: key <span style="color:#080;font-weight:bold">for</span> (key, value) <span style="color:#080">in</span> index.items()}
<span style="color:#080;font-weight:bold">print</span>(index_word[<span style="color:#00d;font-weight:bold">11</span>], index_word[<span style="color:#00d;font-weight:bold">19</span>])
comment = <span style="color:#d20;background-color:#fff0f0">&#39; &#39;</span>.join([index_word.get(i - <span style="color:#00d;font-weight:bold">3</span>, <span style="color:#d20;background-color:#fff0f0">&#39;#&#39;</span>) <span style="color:#080;font-weight:bold">for</span> i <span style="color:#080">in</span> train_data[<span style="color:#00d;font-weight:bold">0</span>]]) <span style="color:#888"># -3 because in the 10.000 words it is missing 2 and the first number is the sentiment</span>
<span style="color:#080;font-weight:bold">print</span>(comment)</code></pre></div>
</div>
<pre class="example">
this film
# this film was just brilliant casting location scenery story direction everyone&#39;s really suited the part they played and you could just imagine being there robert # is an amazing actor and now the same being director # father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for # and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also # to the two little boy&#39;s that played the # of norman and paul they were just brilliant children are often left out of the # list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don&#39;t you think the whole story was so lovely because it was true and was someone&#39;s life after all that was shared with us all
</pre>
</div>
</div>
<div id="outline-container-headline-3" class="outline-2">
<h2 id="headline-3">
Preprocessing
</h2>
<div id="outline-text-headline-3" class="outline-text-2">
<p>Transform the list into 2D tensors with <code class="verbatim">length=maxlen</code>.
So we are limiting the comments to just the first 20 words.
This will affect the accuracy of the final trained neural network model.
We expect a higher accuracy when more words are considered during training.</p>
<div class="src src-python">
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">maxlen = <span style="color:#00d;font-weight:bold">20</span>
train_data = preprocessing.sequence.pad_sequences(train_data, maxlen=maxlen)
test_data = preprocessing.sequence.pad_sequences(test_data, maxlen=maxlen)
<span style="color:#080;font-weight:bold">print</span>(train_data)</code></pre></div>
</div>
<pre class="example">
[[  65   16   38 ...   19  178   32]
 [  23    4 1690 ...   16  145   95]
 [1352   13  191 ...    7  129  113]
 ...
 [  11 1818 7561 ...    4 3586    2]
 [  92  401  728 ...   12    9   23]
 [ 764   40    4 ...  204  131    9]]
</pre>
</div>
</div>
<div id="outline-container-headline-4" class="outline-2">
<h2 id="headline-4">
Build the model
</h2>
<div id="outline-text-headline-4" class="outline-text-2">
<p>
The goal now is to assemble a model that encapsulates the analysis.
A <code class="verbatim">Sequential</code> model is used for a stack of layers in the neural networks.
Each layer has 1 input tensor and 1 output tensor.</p>
<p>
Here we are adding multiple layers to our model.
One of them is the <strong>embedding</strong> layer which turns positive integers into <strong>dense</strong> vectors of fixed sizes.<sup class="footnote-reference"><a id="footnote-reference-1" href="#footnote-1">1</a></sup>
The first argument is the size o the vocabulary (distinct words), in this case we have the first 10000 most common words.
The second argument is the length of the output dense vector, in this case 8.
Finally the input length is the length of each input that will be passed to the model.
Therefore, this layer will convert each comment review data (limited to the first 20) and transform it in a dense vector of length 8.</p>
<p>
Then the <strong>flatten</strong> layer which simply flattens the input, turning it into one dimension.</p>
<p>
Finally we add the <strong>dense</strong> layer.</p>
<p>
The dense layer is the one that we are going to train and use for classification.
It implements a math operation where there is a multiplication between the <strong>input</strong> and <strong>kernel</strong>, then there is a transformation with an activation function.
The kernel is the weight matrix created by the layer.
The activation function used here is the <em>sigmoid</em>, which is a nonlinear function that enables clear prediction.<sup class="footnote-reference"><a id="footnote-reference-2" href="#footnote-2">2</a></sup></p>
<div class="src src-python">
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#080;font-weight:bold">from</span> <span style="color:#b06;font-weight:bold">tensorflow</span> <span style="color:#080;font-weight:bold">import</span> keras
<span style="color:#080;font-weight:bold">from</span> <span style="color:#b06;font-weight:bold">tensorflow.keras</span> <span style="color:#080;font-weight:bold">import</span> layers

model = keras.Sequential()            <span style="color:#888"># model</span>
model.add(layers.Embedding(<span style="color:#00d;font-weight:bold">10000</span>, <span style="color:#00d;font-weight:bold">8</span>, input_length=maxlen))
model.add(layers.Flatten())            <span style="color:#888"># flattens 3D tensor into 2D</span>
model.add(layers.Dense(<span style="color:#00d;font-weight:bold">1</span>, activation=<span style="color:#d20;background-color:#fff0f0">&#39;sigmoid&#39;</span>)) <span style="color:#888"># output with 1 dimension only</span>
model.summary()</code></pre></div>
</div>
<pre class="example">
Model: &#34;sequential_4&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_4 (Embedding)      (None, 20, 8)             80000     
_________________________________________________________________
flatten_4 (Flatten)          (None, 160)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 161       
=================================================================
Total params: 80,161
Trainable params: 80,161
Non-trainable params: 0
_________________________________________________________________
</pre>
</div>
</div>
<div id="outline-container-headline-5" class="outline-2">
<h2 id="headline-5">
Configure the model for training
</h2>
<div id="outline-text-headline-5" class="outline-text-2">
<p>
The <code class="verbatim">compile</code> method configures the model for training.
The <code class="verbatim">optimizer</code> argument specify the algorithm used for this task.
In this example we use the <em>RMSprop</em> algorithm,which means root mean square prop.<sup class="footnote-reference"><a id="footnote-reference-3" href="#footnote-3">3</a></sup>
Optimization in this sense means minimizing a function, in this case we want to minimize the loss function.</p>
<p>
The <code class="verbatim">loss</code> argument is the objective function, or cost function, which indicates how well the model is at predicting for given parameters.
The one chosen was binary-crossentropy, which is a cross entropy loss function with 2 (binary) classes (positive, negative reviews).
The cross-entropy, or log loss, measures performance when the output is a value between 0 or 1 and puts a heavy penalty on lower values which are clearly wrong.</p>
<p>
The <code class="verbatim">metrics</code> argument is a list of things that are going to be tested during training and testing.
Here we used the accuracy.
The accuracy metric is used for classification models and is simply the number of corrected predictions divided by the number of predictions (success rate).</p>
<div class="src src-python">
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">model.compile(optimizer=<span style="color:#d20;background-color:#fff0f0">&#39;rmsprop&#39;</span>, loss=<span style="color:#d20;background-color:#fff0f0">&#39;binary_crossentropy&#39;</span>, metrics=[<span style="color:#d20;background-color:#fff0f0">&#39;acc&#39;</span>])
model.summary()</code></pre></div>
</div>
<pre class="example">
Model: &#34;sequential_4&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_4 (Embedding)      (None, 20, 8)             80000     
_________________________________________________________________
flatten_4 (Flatten)          (None, 160)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 161       
=================================================================
Total params: 80,161
Trainable params: 80,161
Non-trainable params: 0
_________________________________________________________________
</pre>
<p>
From the summary we see the layers.
First we flatten the embedding, then we train a <strong>single dense layer</strong></p>
</div>
</div>
<div id="outline-container-headline-6" class="outline-2">
<h2 id="headline-6">
Training the model
</h2>
<div id="outline-text-headline-6" class="outline-text-2">
<p>
The next step is training the model.
The function performs the number iterations given by <code class="verbatim">epoch</code> and the output is a history of training loss values and metrics at the epochs.
It also shows the validation loss values and validation metrics values.</p>
<div class="src src-python">
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">history = model.fit(train_data, train_targ, <span style="color:#888"># input data and target data </span>
                    epochs=<span style="color:#00d;font-weight:bold">10</span>,         <span style="color:#888"># number of iterations per the data</span>
                    batch_size=<span style="color:#00d;font-weight:bold">32</span>,    <span style="color:#888"># samples per gradient update</span>
                    validation_split=<span style="color:#00d;font-weight:bold">0.2</span>) <span style="color:#888"># fraction of data used for training</span></code></pre></div>
</div>
<pre class="example">
Epoch 1/10
625/625 [==============================] - 2s 3ms/step - loss: 0.6738 - acc: 0.6104 - val_loss: 0.6297 - val_acc: 0.6874
Epoch 2/10
625/625 [==============================] - 1s 2ms/step - loss: 0.5541 - acc: 0.7462 - val_loss: 0.5315 - val_acc: 0.7288
Epoch 3/10
625/625 [==============================] - 1s 2ms/step - loss: 0.4671 - acc: 0.7844 - val_loss: 0.5031 - val_acc: 0.7440
Epoch 4/10
625/625 [==============================] - 2s 2ms/step - loss: 0.4246 - acc: 0.8069 - val_loss: 0.4959 - val_acc: 0.7486
Epoch 5/10
625/625 [==============================] - 1s 2ms/step - loss: 0.3966 - acc: 0.8224 - val_loss: 0.4951 - val_acc: 0.7490
Epoch 6/10
625/625 [==============================] - 1s 2ms/step - loss: 0.3733 - acc: 0.8356 - val_loss: 0.4979 - val_acc: 0.7536
Epoch 7/10
625/625 [==============================] - 1s 2ms/step - loss: 0.3524 - acc: 0.8496 - val_loss: 0.5045 - val_acc: 0.7564
Epoch 8/10
625/625 [==============================] - 2s 2ms/step - loss: 0.3324 - acc: 0.8604 - val_loss: 0.5112 - val_acc: 0.7554
Epoch 9/10
625/625 [==============================] - 1s 2ms/step - loss: 0.3135 - acc: 0.8716 - val_loss: 0.5176 - val_acc: 0.7546
Epoch 10/10
625/625 [==============================] - 1s 2ms/step - loss: 0.2957 - acc: 0.8809 - val_loss: 0.5264 - val_acc: 0.7516
</pre>
</div>
</div>
<div id="outline-container-headline-7" class="outline-2">
<h2 id="headline-7">
Analysis
</h2>
<div id="outline-text-headline-7" class="outline-text-2">
<p>
The training output is a dictionary with the results as a list.</p>
<div class="src src-python">
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#080;font-weight:bold">print</span>(history.history)</code></pre></div>
</div>
<pre class="example">
{&#39;loss&#39;: [0.6738093495368958, 0.5541190505027771, 0.4671066403388977, 0.4246475100517273, 0.3965531885623932, 0.37326815724372864, 0.3524011969566345, 0.3323952257633209, 0.3134663701057434, 0.29573047161102295], &#39;acc&#39;: [0.6103500127792358, 0.746150016784668, 0.7843999862670898, 0.8069499731063843, 0.822350025177002, 0.8355500102043152, 0.8496000170707703, 0.8603500127792358, 0.8715999722480774, 0.8808500170707703], &#39;val_loss&#39;: [0.6296502351760864, 0.5315172076225281, 0.5030556917190552, 0.49588504433631897, 0.4951230585575104, 0.49791309237480164, 0.504469096660614, 0.5111817121505737, 0.5175582766532898, 0.526441216468811], &#39;val_acc&#39;: [0.6873999834060669, 0.7287999987602234, 0.7440000176429749, 0.7486000061035156, 0.7490000128746033, 0.753600001335144, 0.7563999891281128, 0.7554000020027161, 0.7545999884605408, 0.7516000270843506]}
</pre>
<p>
From the training data we can start do some analysis with graphs.
The evolution on the metrics of the validation data (training data) suggests that the prediction power of the neural network model is increasing very fast at the beginning and maintaining a level of the accuracy metric around 76%.
We only used </p>
<div class="src src-python">
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#080;font-weight:bold">import</span> <span style="color:#b06;font-weight:bold">matplotlib.pyplot</span> <span style="color:#080;font-weight:bold">as</span> <span style="color:#b06;font-weight:bold">plt</span>
<span style="color:#080;font-weight:bold">import</span> <span style="color:#b06;font-weight:bold">numpy</span> <span style="color:#080;font-weight:bold">as</span> <span style="color:#b06;font-weight:bold">np</span>
plt.plot(np.arange(<span style="color:#00d;font-weight:bold">1</span>, <span style="color:#038">len</span>(history.history[<span style="color:#d20;background-color:#fff0f0">&#39;val_acc&#39;</span>])+<span style="color:#00d;font-weight:bold">1</span>),
	 history.history[<span style="color:#d20;background-color:#fff0f0">&#39;val_acc&#39;</span>])
plt.xlabel(<span style="color:#d20;background-color:#fff0f0">&#39;Epochs&#39;</span>)
plt.ylabel(<span style="color:#d20;background-color:#fff0f0">&#39;Validation Accuracy&#39;</span>)</code></pre></div>
</div>
<img src="/images/acc.png" alt="/images/acc.png" title="/images/acc.png" width="350px"/>
</div>
</div>
<div id="outline-container-headline-8" class="outline-2">
<h2 id="headline-8">
Footnotes
</h2>
</div>
<div class="footnotes">
<hr class="footnotes-separatator">
<div class="footnote-definitions">
<div class="footnote-definition">
<sup id="footnote-1"><a href="#footnote-reference-1">1</a></sup>
<div class="footnote-body">
<p>The dense vector is a better alternative do sparse vectores obtained with one-hot algorithms. </p>
</div>
</div>
<div class="footnote-definition">
<sup id="footnote-2"><a href="#footnote-reference-2">2</a></sup>
<div class="footnote-body">
<p>Because it goes to 0 or 1 very fast. The activation function just maps the internal product between the input and weights into a fixed interval [0, 1].</p>
</div>
</div>
<div class="footnote-definition">
<sup id="footnote-3"><a href="#footnote-reference-3">3</a></sup>
<div class="footnote-body">
<p><a href="https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#rmsprop">Here</a> is a good reference for this algorithm </p>
</div>
</div>
</div>
</div>

  
  </div>
</article>


    <footer id="footer">
  <div class="footer-left">
    Copyright  &copy; 2021  © {year} 
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
        <li><a href="/">about</a></li>
         
        <li><a href="/notes">notes</a></li>
        
      </ul>
    </nav>
  </div>
</footer>


  </div>
</body>

<link rel="stylesheet" href=/lib/font-awesome/css/all.min.css>
<script src=/lib/jquery/jquery.min.js></script>
<script src=/js/main.js></script>
</html>
