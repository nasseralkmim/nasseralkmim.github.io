<!DOCTYPE html>
<html lang="en-us">
<head>
  <link rel="preload" href="../../../lib/font-awesome/webfonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="preload" href="../../../lib/font-awesome/webfonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="preload" href="../../../lib/font-awesome/webfonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="preload" href="../../../lib/JetBrainsMono/web/woff2/JetBrainsMono-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title> Sentiment analysis on IMDB dataset | Notes repository</title>
  <link rel = 'canonical' href = 'https://nasseralkmim.github.io/notes/sentiment-analysis-python/sentiment-analysis/'>
  <meta name="description" content="This is a page where I collect some of my post-worthy notes and some projects that I have worked on.">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="robots" content="all,follow">
  <meta name="googlebot" content="index,follow,snippet,archive">
  <meta property="og:title" content="Sentiment analysis on IMDB dataset" />
<meta property="og:description" content="Introduction This is as exemple from the excellent book by François Chollet on deep learning." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://nasseralkmim.github.io/notes/sentiment-analysis-python/sentiment-analysis/" /><meta property="article:section" content="notes" />
<meta property="article:published_time" content="2020-12-04T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-12-04T00:00:00+00:00" />

  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Sentiment analysis on IMDB dataset"/>
<meta name="twitter:description" content="Introduction This is as exemple from the excellent book by François Chollet on deep learning."/>

  
  
    
  
  
  <link rel="stylesheet" href="https://nasseralkmim.github.io/css/styles.1443e14ebe36331950cbae7bc52add2aa8581b20a19fd29cf5beed4f163e7d1a3c8fc47b842f22a8559bda282484be39c3d0080eda24c9020d3f0b0d355b7f0d.css" integrity="sha512-FEPhTr42MxlQy657xSrdKqhYGyChn9Kc9b7tTxY&#43;fRo8j8R7hC8iqFWb2igkhL45w9AIDtokyQINPwsNNVt/DQ=="> 

  
  
  
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  

  
<link rel="icon" type="image/png" href="https://nasseralkmim.github.io/images/favicon.ico" />

  
  
  
  
  
    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-74704246-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

  
  
<script src=../../../js/toc.js></script>
</head>

<body class="max-width mx-auto px3 ltr">
  <div class="content index py4">

  <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;" aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
        <li><a href="../../../">home</a></li>
         
        <li><a href="../../../notes">notes</a></li>
         
        <li><a href="../../../tags">tags</a></li>
         
        <li><a href="../../../about">about</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li>
          <a class="icon" href=" https://nasseralkmim.github.io/notes/first-hugo-post/" aria-label="Previous">
            <i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i>
          </a>
        </li>
        
        
        <li>
          <a class="icon" href="https://nasseralkmim.github.io/notes/question-classifier-python/" aria-label="Next">
            <i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i>
          </a>
        </li>
        
        <li>
          <a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" aria-label="Top of Page">
            <i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i>
          </a>
        </li>
        <li>
          <a class="icon" href="#" aria-label="Share">
            <i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i>
          </a>
        </li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      
      <ul>
  
  
    
  
  
  <li>
    <a class="icon" href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fnasseralkmim.github.io%2fnotes%2fsentiment-analysis-python%2fsentiment-analysis%2f" aria-label="Facebook">
      <i class="fab fa-facebook " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://twitter.com/share?url=https%3a%2f%2fnasseralkmim.github.io%2fnotes%2fsentiment-analysis-python%2fsentiment-analysis%2f&text=Sentiment%20analysis%20on%20IMDB%20dataset" aria-label="Twitter">
      <i class="fab fa-twitter " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fnasseralkmim.github.io%2fnotes%2fsentiment-analysis-python%2fsentiment-analysis%2f&title=Sentiment%20analysis%20on%20IMDB%20dataset" aria-label="Linkedin">
      <i class="fab fa-linkedin " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fnasseralkmim.github.io%2fnotes%2fsentiment-analysis-python%2fsentiment-analysis%2f&is_video=false&description=Sentiment%20analysis%20on%20IMDB%20dataset" aria-label="Pinterest">
      <i class="fab fa-pinterest " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="mailto:?subject=Sentiment%20analysis%20on%20IMDB%20dataset&body=Check out this article: https%3a%2f%2fnasseralkmim.github.io%2fnotes%2fsentiment-analysis-python%2fsentiment-analysis%2f" aria-label="Email">
      <i class="fas fa-envelope " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://getpocket.com/save?url=https%3a%2f%2fnasseralkmim.github.io%2fnotes%2fsentiment-analysis-python%2fsentiment-analysis%2f&title=Sentiment%20analysis%20on%20IMDB%20dataset" aria-label="Pocket">
      <i class="fab fa-get-pocket " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://reddit.com/submit?url=https%3a%2f%2fnasseralkmim.github.io%2fnotes%2fsentiment-analysis-python%2fsentiment-analysis%2f&title=Sentiment%20analysis%20on%20IMDB%20dataset" aria-label="reddit">
      <i class="fab fa-reddit " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.tumblr.com/share/link?url=https%3a%2f%2fnasseralkmim.github.io%2fnotes%2fsentiment-analysis-python%2fsentiment-analysis%2f&name=Sentiment%20analysis%20on%20IMDB%20dataset&description=Introduction%20This%20is%20as%20exemple%20from%20the%20excellent%20book%20by%20Fran%c3%a7ois%20Chollet%20on%20deep%20learning." aria-label="Tumblr">
      <i class="fab fa-tumblr " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fnasseralkmim.github.io%2fnotes%2fsentiment-analysis-python%2fsentiment-analysis%2f&t=Sentiment%20analysis%20on%20IMDB%20dataset" aria-label="Hacker News">
      <i class="fab fa-hacker-news " aria-hidden="true"></i>
    </a>
  </li>
</ul>

    </div>
    <div id="toc">
      <nav id="TableOfContents">
<ul>
<li><a href="#headline-1">Introduction</a>
</li>
<li><a href="#headline-2">Load data</a>
</li>
<li><a href="#headline-3">Preprocessing</a>
</li>
<li><a href="#headline-4">Build the model</a>
</li>
<li><a href="#headline-5">Configure the model for training</a>
</li>
<li><a href="#headline-6">Training the model</a>
</li>
<li><a href="#headline-7">Analysis</a>
</li>
<li><a href="#headline-8">Footnotes</a>
</li>
</ul>
</nav>
    </div>
  </span>
</div>


  <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
    <header>
      <h1 class="posttitle" itemprop="name headline">
        Sentiment analysis on IMDB dataset
      </h1>
      <div class="meta">
        
        
        
        
        
        
        
        
        
        <div class="postdate">
          
          <time datetime="2020-12-04 00:00:00 &#43;0000 UTC" itemprop="datePublished">Dec 4 2020</time>
          
        </div>
        
        
        <div class="article-read-time">
          <i class="far fa-clock"></i>
          
          8 minute read
        </div>
        
        
        
        <div class="article-tag">
            <i class="fas fa-tag"></i>
            
            
            <a class="tag-link" href="../../../tags/natural-language-processing" rel="tag">natural-language-processing</a>
            
        </div>
        
      </div>
    </header>

  

    <div class="content" itemprop="articleBody">
      
<div id="outline-container-headline-1" class="outline-2">
<h2 id="headline-1">
Introduction
</h2>
<div id="outline-text-headline-1" class="outline-text-2">
<p>
This is as exemple from the excellent book by François Chollet on deep learning.
My idea here is to further detail the explanation with the code output, which the book does not contain.
And since tensorflow 2.0 was released, I will be using <code class="verbatim">tf.keras</code> instead.</p>
<p>
The goal of this example is</p>
<p>
The steps are</p>
</div>
</div>
<div id="outline-container-headline-2" class="outline-2">
<h2 id="headline-2">
Load data
</h2>
<div id="outline-text-headline-2" class="outline-text-2">
<p>
Load imdb data from keras datasets.
This dataset represents 25000 movie reviews and they are labeled by sentiment.
The format is a list of of length 25000 and each entry is another list that represent a review.
Integers represent the frequency of the word, so we are considering the top 1000 most frequent ones.
We can see the sentiment values is 0 or 1.</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1</span><span><span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">tensorflow.keras</span> <span style="color:#a2f;font-weight:bold">import</span> datasets, preprocessing
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2</span><span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3</span><span>max_features <span style="color:#666">=</span> <span style="color:#666">10000</span>           <span style="color:#080;font-style:italic"># number of words</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4</span><span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5</span><span>(train_data, train_targ), (test_data, test_targ) <span style="color:#666">=</span> datasets<span style="color:#666">.</span>imdb<span style="color:#666">.</span>load_data(
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6</span><span>    num_words<span style="color:#666">=</span>max_features) <span style="color:#080;font-style:italic"># limits to the most common ones </span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7</span><span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8</span><span><span style="color:#a2f">print</span>(<span style="color:#a2f">len</span>(train_data), train_data[<span style="color:#666">0</span>])
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9</span><span><span style="color:#a2f">print</span>(<span style="color:#a2f">set</span>(train_targ))
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10</span><span><span style="color:#a2f">print</span>(train_targ)</span></span></code></pre></div>
</div>
<pre class="example">
25000 [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]
{0, 1}
[1 0 0 ... 0 1 0]
</pre>
<p>
With this bunch of numbers is hard to grasp the real meaning of this data.
Fortunately we can convert it back to text using the index of words also provided by the dataset.</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1</span><span>index <span style="color:#666">=</span> datasets<span style="color:#666">.</span>imdb<span style="color:#666">.</span>get_word_index()
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2</span><span><span style="color:#a2f">print</span>(<span style="color:#a2f">list</span>(index<span style="color:#666">.</span>items())[:<span style="color:#666">5</span>])</span></span></code></pre></div>
</div>
<pre class="example">
[(&#39;fawn&#39;, 34701), (&#39;tsukino&#39;, 52006), (&#39;nunnery&#39;, 52007), (&#39;sonja&#39;, 16816), (&#39;vani&#39;, 63951)]
</pre>
<p>
We can see that the word index is a dictionary where the key is the word and the value is the integer representation.
Because we have the values and want the keys, it is useful to invert this dictionary so we can entry and integer and get out a word.</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1</span><span>index_word <span style="color:#666">=</span> {value: key <span style="color:#a2f;font-weight:bold">for</span> (key, value) <span style="color:#a2f;font-weight:bold">in</span> index<span style="color:#666">.</span>items()}
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2</span><span><span style="color:#a2f">print</span>(index_word[<span style="color:#666">11</span>], index_word[<span style="color:#666">19</span>])
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3</span><span>comment <span style="color:#666">=</span> <span style="color:#b44">&#39; &#39;</span><span style="color:#666">.</span>join([index_word<span style="color:#666">.</span>get(i <span style="color:#666">-</span> <span style="color:#666">3</span>, <span style="color:#b44">&#39;#&#39;</span>) <span style="color:#a2f;font-weight:bold">for</span> i <span style="color:#a2f;font-weight:bold">in</span> train_data[<span style="color:#666">0</span>]]) <span style="color:#080;font-style:italic"># -3 because in the 10.000 words it is missing 2 and the first number is the sentiment</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4</span><span><span style="color:#a2f">print</span>(comment)</span></span></code></pre></div>
</div>
<pre class="example">
this film
# this film was just brilliant casting location scenery story direction everyone&#39;s really suited the part they played and you could just imagine being there robert # is an amazing actor and now the same being director # father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for # and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also # to the two little boy&#39;s that played the # of norman and paul they were just brilliant children are often left out of the # list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don&#39;t you think the whole story was so lovely because it was true and was someone&#39;s life after all that was shared with us all
</pre>
</div>
</div>
<div id="outline-container-headline-3" class="outline-2">
<h2 id="headline-3">
Preprocessing
</h2>
<div id="outline-text-headline-3" class="outline-text-2">
<p>Transform the list into 2D tensors with <code class="verbatim">length=maxlen</code>.
So we are limiting the comments to just the first 20 words.
This will affect the accuracy of the final trained neural network model.
We expect a higher accuracy when more words are considered during training.</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1</span><span>maxlen <span style="color:#666">=</span> <span style="color:#666">20</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2</span><span>train_data <span style="color:#666">=</span> preprocessing<span style="color:#666">.</span>sequence<span style="color:#666">.</span>pad_sequences(train_data, maxlen<span style="color:#666">=</span>maxlen)
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3</span><span>test_data <span style="color:#666">=</span> preprocessing<span style="color:#666">.</span>sequence<span style="color:#666">.</span>pad_sequences(test_data, maxlen<span style="color:#666">=</span>maxlen)
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4</span><span><span style="color:#a2f">print</span>(train_data)</span></span></code></pre></div>
</div>
<pre class="example">
[[  65   16   38 ...   19  178   32]
 [  23    4 1690 ...   16  145   95]
 [1352   13  191 ...    7  129  113]
 ...
 [  11 1818 7561 ...    4 3586    2]
 [  92  401  728 ...   12    9   23]
 [ 764   40    4 ...  204  131    9]]
</pre>
</div>
</div>
<div id="outline-container-headline-4" class="outline-2">
<h2 id="headline-4">
Build the model
</h2>
<div id="outline-text-headline-4" class="outline-text-2">
<p>
The goal now is to assemble a model that encapsulates the analysis.
A <code class="verbatim">Sequential</code> model is used for a stack of layers in the neural networks.
Each layer has 1 input tensor and 1 output tensor.</p>
<p>
Here we are adding multiple layers to our model.
One of them is the <strong>embedding</strong> layer which turns positive integers into <strong>dense</strong> vectors of fixed sizes.<sup class="footnote-reference"><a id="footnote-reference-1" href="#footnote-1">1</a></sup>
The first argument is the size o the vocabulary (distinct words), in this case we have the first 10000 most common words.
The second argument is the length of the output dense vector, in this case 8.
Finally the input length is the length of each input that will be passed to the model.
Therefore, this layer will convert each comment review data (limited to the first 20) and transform it in a dense vector of length 8.</p>
<p>
Then the <strong>flatten</strong> layer which simply flattens the input, turning it into one dimension.</p>
<p>
Finally we add the <strong>dense</strong> layer.</p>
<p>
The dense layer is the one that we are going to train and use for classification.
It implements a math operation where there is a multiplication between the <strong>input</strong> and <strong>kernel</strong>, then there is a transformation with an activation function.
The kernel is the weight matrix created by the layer.
The activation function used here is the <em>sigmoid</em>, which is a nonlinear function that enables clear prediction.<sup class="footnote-reference"><a id="footnote-reference-2" href="#footnote-2">2</a></sup></p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1</span><span><span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">tensorflow</span> <span style="color:#a2f;font-weight:bold">import</span> keras
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2</span><span><span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">tensorflow.keras</span> <span style="color:#a2f;font-weight:bold">import</span> layers
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3</span><span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4</span><span>model <span style="color:#666">=</span> keras<span style="color:#666">.</span>Sequential()            <span style="color:#080;font-style:italic"># model</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5</span><span>model<span style="color:#666">.</span>add(layers<span style="color:#666">.</span>Embedding(<span style="color:#666">10000</span>, <span style="color:#666">8</span>, input_length<span style="color:#666">=</span>maxlen))
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6</span><span>model<span style="color:#666">.</span>add(layers<span style="color:#666">.</span>Flatten())            <span style="color:#080;font-style:italic"># flattens 3D tensor into 2D</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7</span><span>model<span style="color:#666">.</span>add(layers<span style="color:#666">.</span>Dense(<span style="color:#666">1</span>, activation<span style="color:#666">=</span><span style="color:#b44">&#39;sigmoid&#39;</span>)) <span style="color:#080;font-style:italic"># output with 1 dimension only</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8</span><span>model<span style="color:#666">.</span>summary()</span></span></code></pre></div>
</div>
<pre class="example">
Model: &#34;sequential_4&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_4 (Embedding)      (None, 20, 8)             80000     
_________________________________________________________________
flatten_4 (Flatten)          (None, 160)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 161       
=================================================================
Total params: 80,161
Trainable params: 80,161
Non-trainable params: 0
_________________________________________________________________
</pre>
</div>
</div>
<div id="outline-container-headline-5" class="outline-2">
<h2 id="headline-5">
Configure the model for training
</h2>
<div id="outline-text-headline-5" class="outline-text-2">
<p>
The <code class="verbatim">compile</code> method configures the model for training.
The <code class="verbatim">optimizer</code> argument specify the algorithm used for this task.
In this example we use the <em>RMSprop</em> algorithm,which means root mean square prop.<sup class="footnote-reference"><a id="footnote-reference-3" href="#footnote-3">3</a></sup>
Optimization in this sense means minimizing a function, in this case we want to minimize the loss function.</p>
<p>
The <code class="verbatim">loss</code> argument is the objective function, or cost function, which indicates how well the model is at predicting for given parameters.
The one chosen was binary-crossentropy, which is a cross entropy loss function with 2 (binary) classes (positive, negative reviews).
The cross-entropy, or log loss, measures performance when the output is a value between 0 or 1 and puts a heavy penalty on lower values which are clearly wrong.</p>
<p>
The <code class="verbatim">metrics</code> argument is a list of things that are going to be tested during training and testing.
Here we used the accuracy.
The accuracy metric is used for classification models and is simply the number of corrected predictions divided by the number of predictions (success rate).</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1</span><span>model<span style="color:#666">.</span>compile(optimizer<span style="color:#666">=</span><span style="color:#b44">&#39;rmsprop&#39;</span>, loss<span style="color:#666">=</span><span style="color:#b44">&#39;binary_crossentropy&#39;</span>, metrics<span style="color:#666">=</span>[<span style="color:#b44">&#39;acc&#39;</span>])
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2</span><span>model<span style="color:#666">.</span>summary()</span></span></code></pre></div>
</div>
<pre class="example">
Model: &#34;sequential_4&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_4 (Embedding)      (None, 20, 8)             80000     
_________________________________________________________________
flatten_4 (Flatten)          (None, 160)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 161       
=================================================================
Total params: 80,161
Trainable params: 80,161
Non-trainable params: 0
_________________________________________________________________
</pre>
<p>
From the summary we see the layers.
First we flatten the embedding, then we train a <strong>single dense layer</strong></p>
</div>
</div>
<div id="outline-container-headline-6" class="outline-2">
<h2 id="headline-6">
Training the model
</h2>
<div id="outline-text-headline-6" class="outline-text-2">
<p>
The next step is training the model.
The function performs the number iterations given by <code class="verbatim">epoch</code> and the output is a history of training loss values and metrics at the epochs.
It also shows the validation loss values and validation metrics values.</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1</span><span>history <span style="color:#666">=</span> model<span style="color:#666">.</span>fit(train_data, train_targ, <span style="color:#080;font-style:italic"># input data and target data </span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2</span><span>                    epochs<span style="color:#666">=</span><span style="color:#666">10</span>,         <span style="color:#080;font-style:italic"># number of iterations per the data</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3</span><span>                    batch_size<span style="color:#666">=</span><span style="color:#666">32</span>,    <span style="color:#080;font-style:italic"># samples per gradient update</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4</span><span>                    validation_split<span style="color:#666">=</span><span style="color:#666">0.2</span>) <span style="color:#080;font-style:italic"># fraction of data used for training</span></span></span></code></pre></div>
</div>
<pre class="example">
Epoch 1/10
625/625 [==============================] - 2s 3ms/step - loss: 0.6738 - acc: 0.6104 - val_loss: 0.6297 - val_acc: 0.6874
Epoch 2/10
625/625 [==============================] - 1s 2ms/step - loss: 0.5541 - acc: 0.7462 - val_loss: 0.5315 - val_acc: 0.7288
Epoch 3/10
625/625 [==============================] - 1s 2ms/step - loss: 0.4671 - acc: 0.7844 - val_loss: 0.5031 - val_acc: 0.7440
Epoch 4/10
625/625 [==============================] - 2s 2ms/step - loss: 0.4246 - acc: 0.8069 - val_loss: 0.4959 - val_acc: 0.7486
Epoch 5/10
625/625 [==============================] - 1s 2ms/step - loss: 0.3966 - acc: 0.8224 - val_loss: 0.4951 - val_acc: 0.7490
Epoch 6/10
625/625 [==============================] - 1s 2ms/step - loss: 0.3733 - acc: 0.8356 - val_loss: 0.4979 - val_acc: 0.7536
Epoch 7/10
625/625 [==============================] - 1s 2ms/step - loss: 0.3524 - acc: 0.8496 - val_loss: 0.5045 - val_acc: 0.7564
Epoch 8/10
625/625 [==============================] - 2s 2ms/step - loss: 0.3324 - acc: 0.8604 - val_loss: 0.5112 - val_acc: 0.7554
Epoch 9/10
625/625 [==============================] - 1s 2ms/step - loss: 0.3135 - acc: 0.8716 - val_loss: 0.5176 - val_acc: 0.7546
Epoch 10/10
625/625 [==============================] - 1s 2ms/step - loss: 0.2957 - acc: 0.8809 - val_loss: 0.5264 - val_acc: 0.7516
</pre>
</div>
</div>
<div id="outline-container-headline-7" class="outline-2">
<h2 id="headline-7">
Analysis
</h2>
<div id="outline-text-headline-7" class="outline-text-2">
<p>
The training output is a dictionary with the results as a list.</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1</span><span><span style="color:#a2f">print</span>(history<span style="color:#666">.</span>history)</span></span></code></pre></div>
</div>
<pre class="example">
{&#39;loss&#39;: [0.6738093495368958, 0.5541190505027771, 0.4671066403388977, 0.4246475100517273, 0.3965531885623932, 0.37326815724372864, 0.3524011969566345, 0.3323952257633209, 0.3134663701057434, 0.29573047161102295], &#39;acc&#39;: [0.6103500127792358, 0.746150016784668, 0.7843999862670898, 0.8069499731063843, 0.822350025177002, 0.8355500102043152, 0.8496000170707703, 0.8603500127792358, 0.8715999722480774, 0.8808500170707703], &#39;val_loss&#39;: [0.6296502351760864, 0.5315172076225281, 0.5030556917190552, 0.49588504433631897, 0.4951230585575104, 0.49791309237480164, 0.504469096660614, 0.5111817121505737, 0.5175582766532898, 0.526441216468811], &#39;val_acc&#39;: [0.6873999834060669, 0.7287999987602234, 0.7440000176429749, 0.7486000061035156, 0.7490000128746033, 0.753600001335144, 0.7563999891281128, 0.7554000020027161, 0.7545999884605408, 0.7516000270843506]}
</pre>
<p>
From the training data we can start do some analysis with graphs.
The evolution on the metrics of the validation data (training data) suggests that the prediction power of the neural network model is increasing very fast at the beginning and maintaining a level of the accuracy metric around 76%.
We only used </p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1</span><span><span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">matplotlib.pyplot</span> <span style="color:#a2f;font-weight:bold">as</span> <span style="color:#00f;font-weight:bold">plt</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2</span><span><span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">numpy</span> <span style="color:#a2f;font-weight:bold">as</span> <span style="color:#00f;font-weight:bold">np</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3</span><span>plt<span style="color:#666">.</span>plot(np<span style="color:#666">.</span>arange(<span style="color:#666">1</span>, <span style="color:#a2f">len</span>(history<span style="color:#666">.</span>history[<span style="color:#b44">&#39;val_acc&#39;</span>])<span style="color:#666">+</span><span style="color:#666">1</span>),
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4</span><span>	 history<span style="color:#666">.</span>history[<span style="color:#b44">&#39;val_acc&#39;</span>])
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5</span><span>plt<span style="color:#666">.</span>xlabel(<span style="color:#b44">&#39;Epochs&#39;</span>)
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6</span><span>plt<span style="color:#666">.</span>ylabel(<span style="color:#b44">&#39;Validation Accuracy&#39;</span>)</span></span></code></pre></div>
</div>
<img src="../../../images/acc.png" alt="/images/acc.png" title="/images/acc.png" width="350px"/>
</div>
</div>
<div id="outline-container-headline-8" class="outline-2">
<h2 id="headline-8">
Footnotes
</h2>
</div>
<div class="footnotes">
<hr class="footnotes-separatator">
<div class="footnote-definitions">
<div class="footnote-definition">
<sup id="footnote-1"><a href="#footnote-reference-1">1</a></sup>
<div class="footnote-body">
<p>The dense vector is a better alternative do sparse vectores obtained with one-hot algorithms. </p>
</div>
</div>
<div class="footnote-definition">
<sup id="footnote-2"><a href="#footnote-reference-2">2</a></sup>
<div class="footnote-body">
<p>Because it goes to 0 or 1 very fast. The activation function just maps the internal product between the input and weights into a fixed interval [0, 1].</p>
</div>
</div>
<div class="footnote-definition">
<sup id="footnote-3"><a href="#footnote-reference-3">3</a></sup>
<div class="footnote-body">
<p><a href="https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#rmsprop">Here</a> is a good reference for this algorithm </p>
</div>
</div>
</div>
</div>

    </div>
  </article>

   



  
  




  
    <div class="blog-post-comments">
        <div id="disqus_thread">
          <script type="text/javascript">
          
          (function() {
              
              
              
              
          
              var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
              var disqus_shortname = 'nasseralkmim';
              dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
              (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
          })();
          </script>
          <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
          <a href="https://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
        </div>
    </div>

  


  <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="../../../">home</a></li>
         
          <li><a href="../../../notes">notes</a></li>
         
          <li><a href="../../../tags">tags</a></li>
         
          <li><a href="../../../about">about</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <nav id="TableOfContents">
<ul>
<li><a href="#headline-1">Introduction</a>
</li>
<li><a href="#headline-2">Load data</a>
</li>
<li><a href="#headline-3">Preprocessing</a>
</li>
<li><a href="#headline-4">Build the model</a>
</li>
<li><a href="#headline-5">Configure the model for training</a>
</li>
<li><a href="#headline-6">Training the model</a>
</li>
<li><a href="#headline-7">Analysis</a>
</li>
<li><a href="#headline-8">Footnotes</a>
</li>
</ul>
</nav>
    </div>

    <div id="share-footer" style="display: none">
      
      <ul>
  
  
    
  
  
  <li>
    <a class="icon" href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fnasseralkmim.github.io%2fnotes%2fsentiment-analysis-python%2fsentiment-analysis%2f" aria-label="Facebook">
      <i class="fab fa-facebook fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://twitter.com/share?url=https%3a%2f%2fnasseralkmim.github.io%2fnotes%2fsentiment-analysis-python%2fsentiment-analysis%2f&text=Sentiment%20analysis%20on%20IMDB%20dataset" aria-label="Twitter">
      <i class="fab fa-twitter fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fnasseralkmim.github.io%2fnotes%2fsentiment-analysis-python%2fsentiment-analysis%2f&title=Sentiment%20analysis%20on%20IMDB%20dataset" aria-label="Linkedin">
      <i class="fab fa-linkedin fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fnasseralkmim.github.io%2fnotes%2fsentiment-analysis-python%2fsentiment-analysis%2f&is_video=false&description=Sentiment%20analysis%20on%20IMDB%20dataset" aria-label="Pinterest">
      <i class="fab fa-pinterest fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="mailto:?subject=Sentiment%20analysis%20on%20IMDB%20dataset&body=Check out this article: https%3a%2f%2fnasseralkmim.github.io%2fnotes%2fsentiment-analysis-python%2fsentiment-analysis%2f" aria-label="Email">
      <i class="fas fa-envelope fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://getpocket.com/save?url=https%3a%2f%2fnasseralkmim.github.io%2fnotes%2fsentiment-analysis-python%2fsentiment-analysis%2f&title=Sentiment%20analysis%20on%20IMDB%20dataset" aria-label="Pocket">
      <i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://reddit.com/submit?url=https%3a%2f%2fnasseralkmim.github.io%2fnotes%2fsentiment-analysis-python%2fsentiment-analysis%2f&title=Sentiment%20analysis%20on%20IMDB%20dataset" aria-label="reddit">
      <i class="fab fa-reddit fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.tumblr.com/share/link?url=https%3a%2f%2fnasseralkmim.github.io%2fnotes%2fsentiment-analysis-python%2fsentiment-analysis%2f&name=Sentiment%20analysis%20on%20IMDB%20dataset&description=Introduction%20This%20is%20as%20exemple%20from%20the%20excellent%20book%20by%20Fran%c3%a7ois%20Chollet%20on%20deep%20learning." aria-label="Tumblr">
      <i class="fab fa-tumblr fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fnasseralkmim.github.io%2fnotes%2fsentiment-analysis-python%2fsentiment-analysis%2f&t=Sentiment%20analysis%20on%20IMDB%20dataset" aria-label="Hacker News">
      <i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i>
    </a>
  </li>
</ul>

    </div>

    <div id="actions-footer">
      
        <a id="menu-toggle" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;" aria-label="Menu">
          <i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc-toggle" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;" aria-label="TOC">
          <i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share-toggle" class="icon" href="#" onclick="$('#share-footer').toggle();return false;" aria-label="Share">
          <i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" aria-label="Top of Page">
          <i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>


  <footer id="footer">
  <div class="footer-left">
    Copyright  &copy; 2023  Nasser 
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
        <li><a href="../../../">home</a></li>
         
        <li><a href="../../../notes">notes</a></li>
         
        <li><a href="../../../tags">tags</a></li>
         
        <li><a href="../../../about">about</a></li>
        
      </ul>
    </nav>
  </div>
</footer>


  </div>
</body>

<link rel="stylesheet" href=../../../lib/font-awesome/css/all.min.css>
<script src=../../../lib/jquery/jquery.min.js></script>
<script src=../../../js/main.js></script>
<script src=../../../js/code-copy.js></script>


  


<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
<script defer language="javascript" type="text/javascript"  src="../../../js/dynamic-toc.js"></script>

</html>
